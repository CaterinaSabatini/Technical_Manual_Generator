curl http://localhost:11434/api/tags

ollama list      # mostra i modelli installati se il server risponde
ollama ps        # mostra eventuali sessioni/modelli in esecuzione
ollama run llama3.2:latest -p "ping"   # forzatura rapida: se risponde, è su
Test-NetConnection -ComputerName localhost -Port 11434   # Porta API aperta = server attivo
Get-Process | Where-Object {$_.ProcessName -like "ollama*"}  # vedi se il processo c’è

#Per vedere se è in ascolto su localhost
Get-NetTCPConnection -LocalPort 11434 | Select-Object LocalAddress, State

#Per avviarlo
ollama serve

#Se vuoi lasciarlo in background
Start-Process -WindowStyle Hidden ollama serve

#Per pullare il modello se non presente
ollama pull llama3.2:latest


